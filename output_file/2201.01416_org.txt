Latent Vector Expansion using Autoencoder for Anomaly Detection UJu Gim∗, YeongHyeon Park SK Planet Co., Ltd. Keywords: Autoencoder, Anomaly Detection, Deep Learn- ing Abstract Deep learning methods can classify various unstructured data such as images, language, and voice as input data. As the task of classifying anomalies becomes more important in the real world, various methods exist for classifying using deep learn- ing with data collected in the real world. As the task of clas- sifying anomalies becomes more important in the real world, there are various methods for classifying using deep learning with data collected in the real world. Among the various meth- ods, the representative approach is a method of extracting and learning the main features based on a transition model from pre-trained models, and a method of learning an autoencoder- based structure only with normal data and classifying it as ab- normal through a threshold value. However, if the dataset is imbalanced, even the state-of-the-arts models do not achieve good performance. This can be addressed by augmenting nor- mal and abnormal features in imbalanced data as features with strong distinction. We use the features of the autoencoder to train latent vectors from low to high dimensionality. We train normal and abnormal data as a feature that has a strong distinc- tion among the features of imbalanced data. We propose a la- tent vector expansion autoencoder model that improves classi- fication performance at imbalanced data. The proposed method shows performance improvement compared to the basic au- toencoder using imbalanced anomaly dataset. 1 Introduction Nowadays, using deep learning through data sets collected in the real world, image classification [1, 2] and abnormal situa- tion determination [3, 4] among various unstructured data are in progress. In order to classify images and determine abnor- mal situations, data is collected and meaningful feature values are extracted from the collected datasets. The extracted signif- icant feature values enable the machine to recognize the input data of the data set. However, there is a problem that the ma- chine does not recognize the difference between normal and abnormal even if data pre-processing is performed among the collected data, and it is difficult to compare the differences be- tween classes. To overcome this limitation, we train an autoen- coder with a simple change in the structure of the basic autoen- coder [5]. Normal and abnormal data are augmented through ∗Correspondence author: gim.uju1217@sk.com the latent vector generated by the encoder from trained autoen- coder. In order to extract strong feature values for normal and abnormal classification of the generated latent vectors, the low dimensionality is extended to the high dimensionality. This improves classification performance by strongly demarcating the decision boundaries of each class. We show an increase in performance over the basic autoencoder by experimenting with imbalanced anomaly dataset. 2 Related work The basic autoencoder consists of an encoder and a decoder. By reducing the dimension of the input value through the en- coder, the value is compressed into features representative of the input value. After that, the reduced-dimensional latent vec- tor is put as the input value of the decoder and the vector is restored to generate a value similar to the input value of the encoder. The purpose of a basic autoencoder is to train mean- ingful data from a reduced-dimensional latent vector in the en- coder. Variational autoencoder, similar in structure to autoen- coder, expresses probability distribution in input data through latent vectors [6]. After that, data is generated through the de- coder. The difference between an autoencoder and a variational autoencoder is as follows. The autoencoder recovers the re- duced data by reducing the dimensions of the data. Conversely, a variational autoencoder is a generative model that generates similar data in a probability distribution. The reason we use the autoencoder is that it is used only for the purpose of reducing the dimension, not for the purpose of data generation. Contrary to the encoder of the autoencoder, the kernel trick is to extend the characteristics of data from low to high di- mensionality to distinguish the boundary determination of each class [7]. Support vector machine (SVM) [8] improves the clas- sification performance of input data from low-dimensional lin- ear models to high-dimensional models. The kernel trick does not actually expand the data, but rather computes the scalar product of the data through the extended properties. We solve the classification problem by explosively expanding the dimen- sionality of the latent vector of the autoencoder, which is dif- ficult to classify due to the reduced dimensionality of feature values in low dimensions. 3 Latent vector expansion-based autoencoder In this section, we present the latent vector extension-based au- toencoder model, which is the method proposed in this paper. The Basic Autoencoder (BA) in ?? shows the basic autoen- arXiv:2201.01416v1 [cs.CV] 5 Jan 2022 


Figure 1. Comparison of autoencoder structure coder and our autoencoder, which is restored through the de- coder after reducing the dimension of the encoder. Each para- graph describes our autoencoder and Latent Vector Expansion Network. We present our autoencoders different from BA. First, we add the first linear layer to the encoder of the au- toencoder, which specifies the size of the input value data. The second layer is set as a linear layer that reduces in proportion to the size of the first layer, and the input and output values are the same. Add the ReLu activation function to the first and second layers. The third layer of the encoder sets the output value of the second layer as an input value and sets the in- put value of the first layer as an output value. Subsequently, the decoder reverses the parameters of the set layer of the en- coder, and then sets the activation function of the last layer to sigmoid. We adopted ours, which expands the output of the second layer based on the output of the third layer through ex- periments. Table 1 shows the outstanding performance of ours, which expands the output value in the third layer after reduc- ing the size of each input data compared to the latent vector generated through the BA structure. A description of the linear model is given in the next paragraph, Latent Vector Expansion Network. Table 1. Autoencoder comparison with AUROC. Method AUROC Linear model w/ BA 0.872 Linear model w/ Ours 0.951 Second, we construct a Latent Vector Expansion network that trains latent vectors from the trained autoencoder’s en- coder. This network simply consists of two linear layers and a linear model as an output. After setting the input value of the first layer as the output value of the encoder, the output value expands the dimension to 1,024. After that, the activation func- tion uses ReLU and the dropout is set to 0.5. In addition, a lin- ear layer is set to set the output to 1 for normal and abnormal binary classification, and the final value is output through a log sigmoid. We show the 1024-dimensional performance through Table 2. A detailed description of each table is provided in Chapter 4. Table 2. Expansion dimension comparison with AUROC. Method Expansion dimension AUROC Ours w/ expansion 128 0.969 256 0.968 512 0.969 1,024 0.970 4 Experiments In this chapter, we present the dataset and preprocessing used for the experiment of our proposed latent vector expansion- based autoencoder. After that, the parameter settings used in each of Tables 1, 2 and 3 will be described. The experiment used credit card data set, and the main purpose of this data set is to classify fraud. It is an imbalanced data set with 492 ab- normal out of a total of 284,807 data, accounting for only about 0.17% [9]. We performed Min-Max scaling to prevent overfit- ting of imbalanced dataset, and conduct experiments through K-Fold, which is cross validation technic. For all models used in the experiment, the epoch is 20, the learning rate is 0.001, the optimizer used Adam, and the loss used binary cross en- tropy. The difference is that the autoencoder uses Mean Square error for Epoch 50 and loss during training. First in Table 1, w/ BA and w/ Ours set the encoder latent vector as the input value of the linear model after learning the autoencoder. Table 2 conducts the experiment by changing the extension dimension of the first layer of the latent vector extension net- work. Table 3 shows that each model conducts an experiment through the input value of the K-fold training dataset without an autoencoder. A latent vector extension network is used as a linear model. The w/o expansion changed the expansion di- mension of the first layer of the linear model to 10, and the w/ expansion changed the expansion dimension to 1,024 and conducted the experiment. 4.1 Experiment result In this chapter, we explain the reasons for the performance dif- ference between Table 3 and Table 4. We show the BA model and our model performance using K-Fold in Table 4. Com- pared with Table 3, Table 4 has relatively not good performance compared to the linear models, but the input value of the lin- ear model is basic data without reducing the dimension of the data. Since our model is data extracted from the encoder of a trained autoencoder, there are two advantages. The first is that it can be lightweight when learning the model, and the second can be faster than the basic data when inference is performed. In the comparison of models in Table 4, it shows superior per- formance in all Folds compared to when the BA is used. For this reason, our proposed Latent vector expansion-based au- toencoder and latent vector expansion method serve to improve performance. Figure 2 shows the PCA analysis of 2 and 2 folds showing the best performance and 5 and 7 folds showing not good per- 


formance in the two models (left is ours, right is BA in Figure 2) in Table 4. It can be seen that models classifier even the data set with not good data distribution using an expansion latent vector. Table 3. Linear model Comparison of with out expansion and with expansion models. Fold Linear model Linear model w/o expansion w/ expansion AUROC AUROC 1 0.994 0.990 2 0.995 0.995 3 0.998 0.999 4 0.935 0.927 5 0.964 0.972 6 0.986 0.987 7 0.990 0.991 8 0.970 0.978 9 0.983 0.985 10 0.973 0.981 5 Conclusion In this paper, we proposed a latent vector extension-based au- toencoder model that can show that our methodology is su- perior to that of the basic autoencoder structure and encoder for abnormal situation data. Through experiments, the pro- posed model showed that the abnormal situation detection per- formance was superior to the existing model through the credit card dataset. We will future experiment with images and vari- ous data sets. Table 4. BA model and our model performance comparison. Fold BA Ours w/ expansion w/ expansion AUROC AUROC 1 0.939 0.957 2 0.943 0.969 3 0.884 0.930 4 0.851 0.901 5 0.864 0.898 6 0.919 0.945 7 0.790 0.920 8 0.923 0.954 9 0.884 0.910 10 0.847 0.935 [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient- based learning applied to document recognition,” Proceed- ings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. [2] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” 2009. Figure 2. BA model and our model PCA Comparison [3] Y. Park, W. S. Park, and Y. B. Kim, “Anomaly detection in particulate matter sensor using hypothesis pruning genera- tive adversarial network,” ETRI Journal, 2020. [4] W. Sultani, C. Chen, and M. Shah, “Real-world anomaly detection in surveillance videos,” in Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 6479–6488, 2018. [5] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learn- ing algorithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554, 2006. [6] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [7] B. Scholkopf, “The kernel trick for distances,” Advances in neural information processing systems, pp. 301–307, 2001. [8] W. S. Noble, “What is a support vector machine?,” Nature biotechnology, vol. 24, no. 12, pp. 1565–1567, 2006. [9] Y.-A. Le Borgne and G. Bontempi, “Machine learning for credit card fraud detection-practical handbook,” ACM SIGKDD explorations newsletter, vol. 6, no. 1, pp. 1–6, 2004. 


