

==================================== 第1页 ====================================

Latent Vector Expansion using Autoencoder for Anomaly Detection UJu Gim∗, YeongHyeon Park SK Planet Co., Ltd. Keywords: Autoencoder, Anomaly Detection, Deep Learn- ing Abstract Deep learning methods can classify various unstructured data such as images, language, and voice as input data. As the task of classifying anomalies becomes more important in the real world, various methods exist for classifying using deep learn- ing with data collected in the real world. As the task of clas- sifying anomalies becomes more important in the real world, there are various methods for classifying using deep learning with data collected in the real world. Among the various meth- ods, the representative approach is a method of extracting and learning the main features based on a transition model from pre-trained models, and a method of learning an autoencoder- based structure only with normal data and classifying it as ab- normal through a threshold value. However, if the dataset is imbalanced, even the state-of-the-arts models do not achieve good performance. This can be addressed by augmenting nor- mal and abnormal features in imbalanced data as features with strong distinction. We use the features of the autoencoder to train latent vectors from low to high dimensionality. We train normal and abnormal data as a feature that has a strong distinc- tion among the features of imbalanced data. We propose a la- tent vector expansion autoencoder model that improves classi- fication performance at imbalanced data. The proposed method shows performance improvement compared to the basic au- toencoder using imbalanced anomaly dataset. 1 Introduction Nowadays, using deep learning through data sets collected in the real world, image classification [1, 2] and abnormal situa- tion determination [3, 4] among various unstructured data are in progress. In order to classify images and determine abnor- mal situations, data is collected and meaningful feature values are extracted from the collected datasets. The extracted signif- icant feature values enable the machine to recognize the input data of the data set. However, there is a problem that the ma- chine does not recognize the difference between normal and abnormal even if data pre-processing is performed among the collected data, and it is difficult to compare the differences be- tween classes. To overcome this limitation, we train an autoen- coder with a simple change in the structure of the basic autoen- coder [5]. Normal and abnormal data are augmented through ∗Correspondence author: gim.uju1217@sk.com the latent vector generated by the encoder from trained autoen- coder. In order to extract strong feature values for normal and abnormal classification of the generated latent vectors, the low dimensionality is extended to the high dimensionality. This improves classification performance by strongly demarcating the decision boundaries of each class. We show an increase in performance over the basic autoencoder by experimenting with imbalanced anomaly dataset. 2 Related work The basic autoencoder consists of an encoder and a decoder. By reducing the dimension of the input value through the en- coder, the value is compressed into features representative of the input value. After that, the reduced-dimensional latent vec- tor is put as the input value of the decoder and the vector is restored to generate a value similar to the input value of the encoder. The purpose of a basic autoencoder is to train mean- ingful data from a reduced-dimensional latent vector in the en- coder. Variational autoencoder, similar in structure to autoen- coder, expresses probability distribution in input data through latent vectors [6]. After that, data is generated through the de- coder. The difference between an autoencoder and a variational autoencoder is as follows. The autoencoder recovers the re- duced data by reducing the dimensions of the data. Conversely, a variational autoencoder is a generative model that generates similar data in a probability distribution. The reason we use the autoencoder is that it is used only for the purpose of reducing the dimension, not for the purpose of data generation. Contrary to the encoder of the autoencoder, the kernel trick is to extend the characteristics of data from low to high di- mensionality to distinguish the boundary determination of each class [7]. Support vector machine (SVM) [8] improves the clas- sification performance of input data from low-dimensional lin- ear models to high-dimensional models. The kernel trick does not actually expand the data, but rather computes the scalar product of the data through the extended properties. We solve the classification problem by explosively expanding the dimen- sionality of the latent vector of the autoencoder, which is dif- ficult to classify due to the reduced dimensionality of feature values in low dimensions. 3 Latent vector expansion-based autoencoder In this section, we present the latent vector extension-based au- toencoder model, which is the method proposed in this paper. The Basic Autoencoder (BA) in ?? shows the basic autoen- arXiv:2201.01416v1 [cs.CV] 5 Jan 2022 


使用自动编码器进行异常检测的潜在矢量扩展UJu-Gim∗, 永贤公园SK Planet有限公司有限公司关键词：自动编码器、异常检测、深度学习抽象深度学习方法可以将各种非结构化数据（如图像、语言和语音）分类为输入数据。随着对异常进行分类的任务在现实世界中变得越来越重要，存在各种使用深度学习对现实世界中收集的数据进行分类的方法。随着对异常进行分类的任务在现实世界中变得越来越重要，使用深度学习对现实世界中收集的数据进行分类的方法多种多样。在各种方法中，代表性方法是基于预训练模型的过渡模型提取和学习主要特征的方法，以及仅使用正常数据学习基于自动编码器的结构并通过阈值将其分类为异常的方法。然而，如果数据集不平衡，即使是最先进的模型也无法获得良好的性能。这可以通过将不平衡数据中的正常和异常特征增强为具有强烈区别的特征来解决。我们使用自动编码器的特性从低维到高维训练潜在向量。我们将正常和异常数据训练为在不平衡数据的特征之间具有强烈区别的特征。我们提出了一种潜在的矢量扩展自动编码器模型，该模型可以提高不平衡数据的分类性能。与使用不平衡异常数据集的基本自动编码器相比，所提出的方法显示了性能改进。1简介如今，通过在现实世界中收集的数据集进行深度学习，各种非结构化数据中的图像分类[1，2]和异常情况确定[3，4]正在进行中。为了对图像进行分类并确定异常情况，收集数据并从收集的数据集中提取有意义的特征值。提取的重要特征值使机器能够识别数据集的输入数据。然而，存在一个问题，即即使在收集的数据中执行数据预处理，机器也无法识别正常和异常之间的差异，并且很难比较类之间的差异。为了克服这一限制，我们训练了一个自动编码器，只需简单改变基本自动编码器的结构即可[5]。正常和异常数据通过∗通讯作者：gim.uju1217@sk.com编码器从经过训练的自动编码器生成的潜在矢量。为了提取生成的潜在向量的正常和异常分类的强特征值，将低维扩展到高维。这通过强烈划分每个类的决策边界来提高分类性能。通过对不平衡异常数据集的实验，我们显示了比基本自动编码器性能的提高。2相关工作基本的自动编码器由编码器和解码器组成。通过通过编码器减小输入值的维数，该值被压缩成代表输入值的特征。之后，将降维潜在向量作为解码器的输入值，并恢复该向量以生成与编码器的输入值类似的值。基本自动编码器的目的是从编码器中的降维潜在向量训练有意义的数据。变分自动编码器在结构上类似于自动编码器，通过潜在向量表达输入数据中的概率分布[6]。之后，通过解码器生成数据。自动编码器和变分自动编码器之间的区别如下。自动编码器通过减少数据的维数来恢复减少的数据。相反，变分自动编码器是一种生成模型，在概率分布中生成相似的数据。我们使用自动编码器的原因是，它仅用于减少维度，而不是用于数据生成。与自动编码器的编码器相反，核心技巧是将数据的特征从低维扩展到高维，以区分每个类的边界确定[7]。支持向量机（SVM）[8]改进了输入数据从低维线性模型到高维模型的分类性能。内核技巧实际上并不扩展数据，而是通过扩展属性计算数据的标量积。我们通过爆炸性地扩展自动编码器的潜在向量的维数来解决分类问题，由于低维特征值的维数降低，难以分类。3基于潜在向量扩展的自动编码器在本节中，我们提出了基于潜在向量扩张的自动编码器模型，这是本文提出的方法。？？中的基本自动编码器（BA）？？显示了基本autoenarXiv:2201.01416v1[cs.CV]2022年1月5日


==================================== 第2页 ====================================

Figure 1. Comparison of autoencoder structure coder and our autoencoder, which is restored through the de- coder after reducing the dimension of the encoder. Each para- graph describes our autoencoder and Latent Vector Expansion Network. We present our autoencoders different from BA. First, we add the first linear layer to the encoder of the au- toencoder, which specifies the size of the input value data. The second layer is set as a linear layer that reduces in proportion to the size of the first layer, and the input and output values are the same. Add the ReLu activation function to the first and second layers. The third layer of the encoder sets the output value of the second layer as an input value and sets the in- put value of the first layer as an output value. Subsequently, the decoder reverses the parameters of the set layer of the en- coder, and then sets the activation function of the last layer to sigmoid. We adopted ours, which expands the output of the second layer based on the output of the third layer through ex- periments. Table 1 shows the outstanding performance of ours, which expands the output value in the third layer after reduc- ing the size of each input data compared to the latent vector generated through the BA structure. A description of the linear model is given in the next paragraph, Latent Vector Expansion Network. Table 1. Autoencoder comparison with AUROC. Method AUROC Linear model w/ BA 0.872 Linear model w/ Ours 0.951 Second, we construct a Latent Vector Expansion network that trains latent vectors from the trained autoencoder’s en- coder. This network simply consists of two linear layers and a linear model as an output. After setting the input value of the first layer as the output value of the encoder, the output value expands the dimension to 1,024. After that, the activation func- tion uses ReLU and the dropout is set to 0.5. In addition, a lin- ear layer is set to set the output to 1 for normal and abnormal binary classification, and the final value is output through a log sigmoid. We show the 1024-dimensional performance through Table 2. A detailed description of each table is provided in Chapter 4. Table 2. Expansion dimension comparison with AUROC. Method Expansion dimension AUROC Ours w/ expansion 128 0.969 256 0.968 512 0.969 1,024 0.970 4 Experiments In this chapter, we present the dataset and preprocessing used for the experiment of our proposed latent vector expansion- based autoencoder. After that, the parameter settings used in each of Tables 1, 2 and 3 will be described. The experiment used credit card data set, and the main purpose of this data set is to classify fraud. It is an imbalanced data set with 492 ab- normal out of a total of 284,807 data, accounting for only about 0.17% [9]. We performed Min-Max scaling to prevent overfit- ting of imbalanced dataset, and conduct experiments through K-Fold, which is cross validation technic. For all models used in the experiment, the epoch is 20, the learning rate is 0.001, the optimizer used Adam, and the loss used binary cross en- tropy. The difference is that the autoencoder uses Mean Square error for Epoch 50 and loss during training. First in Table 1, w/ BA and w/ Ours set the encoder latent vector as the input value of the linear model after learning the autoencoder. Table 2 conducts the experiment by changing the extension dimension of the first layer of the latent vector extension net- work. Table 3 shows that each model conducts an experiment through the input value of the K-fold training dataset without an autoencoder. A latent vector extension network is used as a linear model. The w/o expansion changed the expansion di- mension of the first layer of the linear model to 10, and the w/ expansion changed the expansion dimension to 1,024 and conducted the experiment. 4.1 Experiment result In this chapter, we explain the reasons for the performance dif- ference between Table 3 and Table 4. We show the BA model and our model performance using K-Fold in Table 4. Com- pared with Table 3, Table 4 has relatively not good performance compared to the linear models, but the input value of the lin- ear model is basic data without reducing the dimension of the data. Since our model is data extracted from the encoder of a trained autoencoder, there are two advantages. The first is that it can be lightweight when learning the model, and the second can be faster than the basic data when inference is performed. In the comparison of models in Table 4, it shows superior per- formance in all Folds compared to when the BA is used. For this reason, our proposed Latent vector expansion-based au- toencoder and latent vector expansion method serve to improve performance. Figure 2 shows the PCA analysis of 2 and 2 folds showing the best performance and 5 and 7 folds showing not good per- 


图1.自动编码器结构编码器和我们的自动编码器的比较，在减小编码器的尺寸后通过解码器恢复。每一段都描述了我们的自动编码器和潜在向量扩展网络。我们展示了不同于BA的自动编码器。首先，我们将第一个线性层添加到自动编码器的编码器中，该编码器指定输入值数据的大小。第二层被设置为线性层，其与第一层的大小成比例地减小，并且输入和输出值相同。将ReLu激活功能添加到第一层和第二层。编码器的第三层将第二层的输出值设置为输入值，并将第一层的输入值设置为输出值。随后，解码器反转编码器的设置层的参数，然后将最后一层的激活函数设置为S形。我们采用了我们的方法，通过实验在第三层输出的基础上扩展了第二层的输出。表1显示了我们的出色性能，与通过BA结构生成的潜在向量相比，在减小每个输入数据的大小后，第三层中的输出值得到了扩展。线性模型的描述将在下一段，潜在向量扩展网络中给出。表1.自动编码器与AUROC的比较。方法AUROC线性模型w/BA 0.872线性模型w/Ours 0.951第二，我们构建了一个潜在向量扩展网络，从训练的自动编码器的编码器中训练潜在向量。该网络简单地由两个线性层和作为输出的线性模型组成。在将第一层的输入值设置为编码器的输出值之后，输出值将维度扩展到1024。之后，激活函数使用ReLU，并将丢失设置为0.5。此外，线性层被设置为将输出设置为1，用于正常和异常二进制分类，最终值通过log sigmoid输出。我们通过表2展示了1024维的性能。第4章表2提供了每个表的详细描述。与AUROC的扩展维度比较。方法扩展维度AUROC Ours w/扩展128 0.969 256 0.968 512 0.969 1024 0.970 4实验在本章中，我们介绍了用于我们提出的基于潜在向量扩展的自动编码器实验的数据集和预处理。之后，将描述表1、2和3中的每一个中使用的参数设置。实验使用了信用卡数据集，该数据集的主要目的是对欺诈行为进行分类。这是一个不平衡的数据集，在总共284807个数据中有492个异常，仅占约0.17%[9]。我们执行了最小-最大缩放以防止不平衡数据集的过度拟合，并通过K-Fold（交叉验证技术）进行了实验。对于实验中使用的所有模型，历元为20，学习率为0.001，优化器使用Adam，损失使用二进制交叉熵。不同的是，自动编码器使用Epoch 50的均方误差和训练期间的损失。首先在表1中，w/BA和w/Ours在学习自动编码器之后将编码器潜在向量设置为线性模型的输入值。表2通过改变潜在向量扩展网络的第一层的扩展维度来进行实验。表3显示，每个模型在没有自动编码器的情况下通过K倍训练数据集的输入值进行实验。潜在向量可拓网络被用作线性模型。w/o展开将线性模型的第一层的展开尺寸更改为10，w/o膨胀将展开尺寸更改成1024，并进行了实验。4.1实验结果在本章中，我们解释了表3和表4之间性能差异的原因。我们在表4中使用K-Fold展示了BA模型和我们的模型性能。与表3相比，表4与线性模型相比性能相对较差，但线性模型的输入值是基本数据，而没有降低数据的维数。由于我们的模型是从经过训练的自动编码器的编码器中提取的数据，因此有两个优点。第一个是在学习模型时它可以是轻量级的，第二个是在进行推理时可以比基本数据更快。在表4中的模型比较中，与使用BA时相比，它在所有折叠中表现出优异的性能。因此，我们提出的基于潜在向量扩展的自动编码器和潜在向量扩展方法有助于提高性能。图2显示了2倍和2倍的PCA分析，显示了最佳性能，5倍和7倍显示了不好


==================================== 第3页 ====================================

formance in the two models (left is ours, right is BA in Figure 2) in Table 4. It can be seen that models classifier even the data set with not good data distribution using an expansion latent vector. Table 3. Linear model Comparison of with out expansion and with expansion models. Fold Linear model Linear model w/o expansion w/ expansion AUROC AUROC 1 0.994 0.990 2 0.995 0.995 3 0.998 0.999 4 0.935 0.927 5 0.964 0.972 6 0.986 0.987 7 0.990 0.991 8 0.970 0.978 9 0.983 0.985 10 0.973 0.981 5 Conclusion In this paper, we proposed a latent vector extension-based au- toencoder model that can show that our methodology is su- perior to that of the basic autoencoder structure and encoder for abnormal situation data. Through experiments, the pro- posed model showed that the abnormal situation detection per- formance was superior to the existing model through the credit card dataset. We will future experiment with images and vari- ous data sets. Table 4. BA model and our model performance comparison. Fold BA Ours w/ expansion w/ expansion AUROC AUROC 1 0.939 0.957 2 0.943 0.969 3 0.884 0.930 4 0.851 0.901 5 0.864 0.898 6 0.919 0.945 7 0.790 0.920 8 0.923 0.954 9 0.884 0.910 10 0.847 0.935 [1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient- based learning applied to document recognition,” Proceed- ings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. [2] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” 2009. Figure 2. BA model and our model PCA Comparison [3] Y. Park, W. S. Park, and Y. B. Kim, “Anomaly detection in particulate matter sensor using hypothesis pruning genera- tive adversarial network,” ETRI Journal, 2020. [4] W. Sultani, C. Chen, and M. Shah, “Real-world anomaly detection in surveillance videos,” in Proceedings of the IEEE conference on computer vision and pattern recog- nition, pp. 6479–6488, 2018. [5] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learn- ing algorithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp. 1527–1554, 2006. [6] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [7] B. Scholkopf, “The kernel trick for distances,” Advances in neural information processing systems, pp. 301–307, 2001. [8] W. S. Noble, “What is a support vector machine?,” Nature biotechnology, vol. 24, no. 12, pp. 1565–1567, 2006. [9] Y.-A. Le Borgne and G. Bontempi, “Machine learning for credit card fraud detection-practical handbook,” ACM SIGKDD explorations newsletter, vol. 6, no. 1, pp. 1–6, 2004. 


表4中两个模型（左边是我们的，右边是图2中的BA）的性能。可以看出，模型甚至使用扩展潜在向量对数据分布不好的数据集进行分类。表3.线性模型无膨胀模型和膨胀模型的比较。折叠线性模型线性模型无膨胀无膨胀AUROC AUROC 1 0.994 0.990 2 0.995 0.995 3 0.998 0.999 4 0.935 0.927 5 0.964 0.972 6 0.986 0.987 7 0.990 0.991 8 0.970 0.978 9 0.983 0.985 10 0.973 0.981 5结论在本文中，我们提出了一种基于潜在向量扩展的自动编码器模型，可以表明我们的方法优于基本的自动编码器结构和异常情况数据编码器。通过实验，所提出的模型表明，通过信用卡数据集，异常情况检测性能优于现有模型。我们将在未来对图像和各种数据集进行实验。表4.BA模型和我们的模型性能比较。Fold BA Ours w/expansion w/expension AUROC AUROC 1 0.939 0.957 2 0.943 0.969 3 0.884 0.930 4 0.851 0.901 5 0.864 0.898 6 0.919 0.945 7 0.790 0.920 8 0.923 0.954 9 0.884 0.910 10 0.847 0.935[1]Y.LeCun，L.Bottou，Y.Bengio和P.Haffner，“基于梯度的学习应用于文档识别”，《IEEE学报》，第86卷，第11期，第2278–23241998页。[2]A.Krizhevsky，G.Hinton，等人，“从微小图像中学习多层特征”，2009年。图2.BA模型和我们的模型PCA比较[3]Y.Park、W.S.Park和Y.B.Kim，“使用假设剪枝生成对抗网络的颗粒物传感器异常检测”，ETRI杂志，2020年。[4]W.Sultani、C.Chen和M.Shah，“监控视频中的真实世界异常检测”，《IEEE计算机视觉和模式识别会议论文集》，第6479–6488页，2018年。[5]G.E.Hinton、S.Osindero和Y.-W。Teh，“深度信念网络的快速学习算法”，《神经计算》，第18卷，第7期，第1527–1554页，2006年。[6]D.P.Kingma和M.Welling，“自动编码变分贝叶斯”，arXiv预印本arXiv:1312.6114，2013年。[7]B.Scholkopf，“距离的核技巧”，神经信息处理系统进展，第301–307页，2001年。[8]W.S.Noble，“什么是支持向量机？”，《自然生物技术》，第24卷，第12期，第1565-1567页，2006年。Le Borgne和G.Bontempi，“信用卡欺诈检测实用手册的机器学习”，ACM SIGKDD探索通讯，第6卷，第1期，第1-6页，2004年。
